
== Key Findings

This chapter reviews architectural aspects that have been discussed at the Hackathon most intensively. Some of them identify potential alternative approaches to the solutions developed in Testbed-13, others help to answer open questions that had been identified at the end of Testbed-13.

=== Existing Architecture

The overall experience with the implementation based on Testbed-13 results are good.

==== WPS

It was generally agreed that it makes perfect sense to facade any type of application with a WPS. Though, mixed opinions have been stated on the actual location of the WPS, which could be a service offered by the ADES, a service that is available per MEP, or integral part of the Docker container itself (see <<microservice_based_approach, microservice based approach>>). The WPS-based solutions all worked well and provided sufficient flexibility.

===== WPS RESTful

It was emphasized that all participants prefer an OpenAPI 3.0 RESTful interface rather than the Service Oriented Architecture Remote Procedure Call approach described in the http://docs.opengeospatial.org/is/14-065/14-065.html[current standard WPS 2.0]. Participants said that it is much easier and better supported by available tools to interact with a WPS instance that way.

===== WPS Interpretation of New Process Deployment

It turned out that the WPS mechanisms to deploy or register new processes have been perceived differently by the participants. In principle, two options exist to add a new process to an WPS instance: First, to use a dedicated operation, second, to use the Execute operation with specific parameterization.

WPS operations are standard built-in functions supported at the service level (e.g., _GetCapabilities, DescribeProcess,_ or _Execute_). In a transactional WPS, these can be extended by additional operations such as _Deploy_ and _Undeploy_ to allow registering new processes. WPS processes are application specific functions only available in certain WPS instances (in their process offerings). Additional processes can also be added by using the Execute operation. Then, the _Execute(New Process)_ operation gets specific parameters that indicate that this operation execution shall add a new process to the WPS instance. Due to timing constraints, the latter approach was used in the Hackathon, though the used approach has no influence on the actual interpretation problem.

As said, it turned out that the _WPS Deploy()_ respectively _Execute(NewProcess)_ operation was perceived differently by the participants. The intention of these calls is to add a new process to a WPS instance. The WPS advertises this new process as part of its capabilities (section _ProcessOfferings_). Confusion was caused by different interpretations what the WPS is actually doing in the background when adding a new process. In most cases, the WPS might just add this new process to the list of available processes without any further actions. Only when the new process is executed, then the WPS will run the necessary steps to deploy the process on the cloud and starts its execution. Some participants interpreted the word _deploy_ in the sense of actual deployment in the cloud, which would cause a whole lot of additional overhead, such as instance control and monitoring in the cloud. It is emphasized that the behavior of the WPS is totally opaque in that sense. Any WPS instance can do with the new process whatever deemed necessary. It is only required that the new process is offered to WPS users.

==== Application Package

The Application OWS Context Document (OWC) should have a clear, minimum set of mandatory fields required for an application to be valid. This minimum set should than be validated and accepted on all clusters.

Each process should have a unique process identifier. The process identifier is provided within the Application Package (AP) in the <ows:Identifier> element, so it is not generated by the WPS instance usually. It would be possible to generate unique identifiers on the AP production side by using e.g., the SHA-256 message digest of the OWC Application context document. This would help to have a unique, traceable and interchangeable mapping between the OWC document and the process identifier.

*OWS-Context vs. WPD vs. QaDJSON*

Testbed-13 discussed three options to encode an implementation package:

[class=steps]
. Using OWS-Context with embedded Web Processing Service ProcessDescriptor elements;

. Using Web Processing Service ProcessDescriptor (WPD) elements exclusively, i.e. avoid OWS-Context; and

. Using a Quick and Dirty JSON (QaDJSON) representation, either based on some JSON Schema or following conventions.

Then first approach uses OWS-Context to encode the AP. WPS ProcessDescriptor elements are embedded. This approach has the advantage that:

* the developer can add any additional information to the AP that can be used by sophisticated clients to visualize e.g., background maps as part of the graphical consumer interaction interface;

* the developer can add specific mappings to the AP file which might be necessary in order to work with different catalogs (that the application consumer can use to identify the data to be processed). The mappings are necessary because many OpenSearch-based catalogs use different terminology for both request and response parameter (see section <<opensearch,OpenSearch Based Catalogs>> for further details).

* We remain independent of WPS. If, for whatever reason, we need to remove WPS in the future to replace it with some new technology, this can be easily done without breaking the overall concept of APs and AP-handling services.

On the disadvantage side, one can argue that OWS-Context adds additional complexity to the AP that is not absolutely necessary. It requires the AP developer to read and understand yet another standard without gaining much. After some discussion, this aspect has been solved by agreeing on two aspects. Either tools will be developed that guide the AP developer through the process. Think of a kind of wizard that requests essential elements from the AP developer and produces the AP as a result. In this case, the application developer never sees the actual AP code. Alternatively, application developers will follow examples of existing APs and realize that the OWS-Context add some elements, but do not cause any harm during AP development. In any case, we need to have a feedback channel for AP developers to ensure that the additional flexibility gained with OWS-Context comes at minimum costs for the AP developer.

=== Alternative Solutions

==== Pre-Defined Product Execution

An interesting alternative that was presented by defining a single Docker image loaded with major image processing software like GDAL, OrfeoToolBox and SNAP, instead of providing a link to a Docker image including a processing command. This image can then be reused for various processes, as described <<application_package, here>>. The approach would lead to modifications in the Application Package model and may lack from version incompatibilities of the used tools and dependent libraries, but is certainly worth further experimentation.

[[microservice_based_approach]]
==== Microservice Approach

The currently favored architectural approach uses Application Package for application description and deployment; and WPS for process registration, deployment and execution. It has been developed based on the target to minimize extra work for the application developer. All the application developer needs to do is to run a simple command line command to package the application in a Docker container, to upload this container to a Docker hub and to provide the application package.

A very interesting alternative approach follows a different paradigm. In the microservice approach, the developer is familiar with WPS and provides everything required to execute the application in the cloud programmatically. Using IAAS/PAAS solutions such as Kubernetes or Amazon, the user produces a Docker container with a single process WPS embedded. This solution allows the application developer to take care of all data mountings and mappings following his own preferences. The Docker container could then be deployed on cloud platforms.

The approach was developed by several participants, e.g., by VITO as described in more detail in <<vito, this chapter>>, or <<space_applications, Space Applications>>. Potential disadvantages of that approach, including parallelization and resource scheduling issues as well as versioning issues, have been identified by <<eurac_proposed_alternatives, Eurac>>.

=== Further Observations 

[[opensearch]]
==== OpenSearch Based Catalogs

It turned out that many OpenSearch based catalogs use different terminology for both request and response parameters. As an example, some catalogs require the request parameter "start", where others require it to be "startDate". The same applies to the responses, as illustrated in the following examples.

The following queries are requesting the same data. Note the different parameter names: VITO: http://www.vito-eodata.be/openSearch_all/findProducts?collection=urn:eop:VITO:CGS_S1_SLC_L1&start=2018-01-01

CloudFerro: http://finder.eocloud.eu/resto/api/collections/Sentinel1/search.atom?startDate=2018-01-01&_pretty=true

This issue can be mitigated by using URL templates as provided in the OpenSearch Description (OSD) documents. Given that only the name of the placeholders is standardized (such as searchTerms, geo:box, time:start, time:end), not the rest of the URL (which is completely free, including the base path and the name of the parameters), a link to an OSD document could be provided together with the list of key/values pairs for substituting the search placeholders. This link to would be provided instead of providing an OpenSearch request directly.

A major issue are inhomogeneous responses. It is currently impossible for a developer to produce a response module that could process responses from arbitrary catalogs. The response structure is always different, the response terminology is non-homogeneous, and the actual paths to the data differ from one cloud platform to the other. OWS-Context provides some mitigation options, as at least terminology mappings can be described, but standardization should address this issue in a more robust way by either developing a set of conventions, or even better to include commonly used terms in the specification directly. It cannot be expected that mediation and mapping technologies coming from the Semantic Web can solve this issue within the foreseeable future.

==== Networks, Firewalls, CORS, Security

Network connectivity (firewalls, blocked ports) and CORS (Cross-Site Scripting protection) issues took the first half day to be resolved. In particular CORS needs to be resolved on a general level, as it affects all secured interactions between clients and servers and can be considered a general issue of Spatial Data Infrastructures. It is not specific to the technology or setup used in the Hackathon.

In terms of general security (which was not in focus of the Hackathon), initial tests have shown that it requires solid best practices to setup a new secure WPS instance as part of an application-in-the-cloud-deployment scenario. Currently, security settings and requirements are very heterogeneous, thus making it hard to implement secure solutions.

==== Kubernetes, Fargate, EKS, Mesos, Docker, Marathon

There is lots of dynamic in the cloud platform market at the moment. Tools that have been state of the art just a little while ago a now superseded by tools that are even simpler to use, more user friendly, better scalable, cheaper, more robust, or better integrated with other services. At the Hackathon, several of these technologies have been discussed. The following table gives a brief overview of these technologies. Discussing and comparing these technologies is not simple. There are lots of articles or blog posts and an endless amount of social chatter available describing and comparing these technologies. You find a lot of information, often infused with marketing jargon and consultation offers, on what some call the fight-to-the death for container supremacy whereas others claim that the various technologies often solve different things and are rooted in very different contexts. If there is one thing to learn, then certainly to be very careful before building any complex system that is strictly bound to one or a specific set of technologies, tools, or platforms.

.cloud technologies
|===

| https://www.docker.com/[Docker] | Docker groups some of the capabilities of Linux cgroups with namespaces into a single and easy to use package that allows applications to run consistently on any infrastructure. The package is called the Docker image, which allows to package the application together with required libraries into a single container.

| https://aws.amazon.com/eks/[EKS] | Amazon Elastic Container Service for Kubernetes (Amazon EKS) is a managed Kubernetes service. It uses Amazon https://aws.amazon.com/iam/[Identity and Access Management (IAM)] for role based access control, https://aws.amazon.com/about-aws/whats-new/2017/11/introducing-aws-privatelink-for-aws-services/[PrivateLink] to reach your masters, and Amazon Virtual Private Cloud (VPC) for pod networking.

| https://aws.amazon.com/fargate/[Fargate] | Fargate is an Amazon technology to run containers, either orchestrated by ECS or Kubernetes on EKS, without having to manage the underlying servers or clusters (i.e. EC2 instances.), which makes containers a first-class resource.

| https://kubernetes.io/[Kubernetes] | Kubernetes is a clustered container orchestration system that automates the creation, replication, scaling and management of Docker containers.

| https://mesosphere.github.io/marathon/[Marathon] | Marathon is a production-grade container orchestration platform for Mesosphere’s Datacenter Operating System (DC/OS) and Apache Mesos.

| https://mesos.apache.org/[Mesos] | Apache Mesos abstracts CPU, memory, storage, and other compute resources away from machines (physical or virtual), enabling fault-tolerant and elastic distributed systems to easily be built and run effectively.

|===

When investigating favored technologies that are used in applied research projects or system developments, it seems that Docker is the common denominator. Docker containers running in production environments need to be orchestrated across multiple machines; and with the orchestration engines things divert. One of the first orchestration engines was https://mesosphere.github.io/marathon/[Marathon] on top of https://mesos.apache.org/[Apache Mesos]. Now we do have https://www.nomadproject.io/[Nomad], http://kubernetes.io/[Kubernetes], or https://docs.docker.com/engine/swarm/[Docker Swarm], which is now part of Docker Engine.

The container technology Docker (i.e., the file format and runtime engine) was quickly complemented with additional technologies, such as Docker Hub, Docker Registry, Docker Cloud or Docker Datacenter. Docker Hub allows public storage of Docker images whereas the Docker Registry can be used for storing it on-premise. Docker Cloud is a managed service for building and running containers and the Docker Datacenter is a commercial offering embodying many Docker technologies. And quickly we are on the market. That’s where the jargon and market speak starts and true believers espousing their faith are burning heretics who would dare to consider alternatives.

Kubernetes is a technology introduced by Google. It allows to orchestrate Docker containers without having to interact with the underlying infrastructure. It provides a standard deployment interface and primitives for a consistent app deployment experience and APIs across clouds, which can be very useful in our context of deploying and executing applications working on Earth observation data stored on mission exploitation platforms. Kubernetes modular API allows to integrate systems around the core Kubernetes technology. Experiences with Kubernetes, that is now offered as a service by many vendors, show that it could be a viable solution for EO Exploitation Platforms. It would free the application developer from the underlying infrastructure, though it is not quite clear what effort would be required on the platform side to setup and run Kubernetes deployments.

An alternative for infrastructure abstraction is Apache Mesos.
